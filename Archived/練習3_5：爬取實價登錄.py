# -*- coding: utf-8 -*-
"""練習3-5：爬取實價登錄.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z-WeC_5K6EDG9z5Jae1d5l-kky7XqnL4
"""

#安裝相關函式庫
#!pip install requests
#!pip install beautifulsoup4

#匯入必要函式庫
import time
import re
import ntpath
import requests
import csv
from lxml import etree
from google.colab import files

#可以從瀏覽器的開發者工具取得 request 的 header 資訊
headers_Get = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }

#使用 Google 新聞，缺點是可能取到不是內容頁面
baseUrl = 'https://price.houseprice.tw/list/%E5%B1%8F%E6%9D%B1%E7%B8%A3_city/%E5%85%A7%E5%9F%94%E9%84%89_zip/'
#商業智慧+site:www.cw.com.tw
keyword = "屏東內埔"
#site = 'site:www.cw.com.tw'
#queryString = keyword + '+' + site
startItem = 1
startParam = '?p='
#suffix = '&aqs=chrome..69i57j0i333.5056j0j7&sourceid=chrome&ie=UTF-8'

#解析回傳的列表標題及連結
pageLinks = []
contain = 'article'

#請求網頁
def requestHtml(url):
  s = requests.Session()
  r = s.get(url, headers=headers_Get)
  return etree.HTML(r.text)

#組合列表中的標題
def composeItems(links, titles):
  for idx, link in enumerate(links):
    print(link)
    print(titles[idx])
    pageLinks.append({"title":titles[idx], "link":link})

#取得最後一頁
def findLastPage(searchPages):
  lastPage = 1
  for searchPage in searchPages:
    try:
      lastPage = int(searchPage)
      print(lastPage)
    except ValueError:
      print(searchPage + " is not integer")
  return lastPage

#清空字串內全部的 html tag，只留下內文
TAG_RE = re.compile(r'<[^>]+>')
def remove_tags(text):
    return TAG_RE.sub('', text)

#解析檔案路徑及檔名
def path_leaf(path):
    head, tail = ntpath.split(path)
    return tail #or ntpath.basename(head)

#以分包的方法下載檔案
def download_file(url, filename):
    # NOTE the stream=True parameter below
    try:
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(filename, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192): 
                    # If you have chunk encoded response uncomment if
                    # and set chunk_size parameter to None.
                    #if chunk: 
                    f.write(chunk)

        files.download(filename)
        return filename
    except:
        print("Error")
        return 0

transactions = []
def composeTransaction(dom):
  transaction_dates = dom.xpath('(//tr["@data-sid"]/td[1])/text()')
  addresses = dom.xpath('(//tr["@data-sid"]/td[2])/div/p/text()')
  transaction_types = dom.xpath('(//tr["@data-sid"]/td[3])/text()')
  totals = dom.xpath('(//tr["@data-sid"]/td[4])/p/span/text()')
  prices = dom.xpath('(//tr["@data-sid"]/td[5])/div/p/span/text()')
  level_grounds = dom.xpath('(//tr["@data-sid"]/td[6])/p/span/text()')
  floors = dom.xpath('(//tr["@data-sid"]/td[7])/text()')
   
  for idx, transaction_date in enumerate(transaction_dates):
    try:
      transactions.append({
        "transaction_date":transaction_date, 
        "address":addresses[idx], 
        "transaction_type":transaction_types[idx], 
        "total":totals[idx], 
        "price":prices[idx],
        "level_ground":level_grounds[idx],
        "floor":floors[idx]
        })
    except:
      print("Error index:" + str(idx))

#取得最後一頁的數值
lastPage = 100000

#逐頁爬取文章連結及標題
for i in range(1, lastPage):
  try:
    fullUrl = baseUrl + startParam + str(i)
    dom = requestHtml(fullUrl)
    composeTransaction(dom)
  except:
    print("No page found:"+ str(i))
    break

  time.sleep(2)
  break

#輸出CSV
csv_columns = ["transaction_date", 
        "address", 
        "transaction_type", 
        "total", 
        "price",
        "level_ground",
        "floor"]
csv_file = keyword + ".csv"
try:
    with open(csv_file, 'w') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)
        writer.writeheader()
        for transaction in transactions:
            writer.writerow(transaction)
except IOError:
    print("CSV 檔案寫入發生錯誤")

