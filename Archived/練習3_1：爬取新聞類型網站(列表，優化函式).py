# -*- coding: utf-8 -*-
"""練習3-1：爬取新聞類型網站(列表，優化函式).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Xh1IR3W01Vz5XTR2G1y-tTvORdlAKAP
"""

!pip install requests
!pip install beautifulsoup4

import requests
from bs4 import BeautifulSoup

headers_Get = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }

#使用 Google 新聞，缺點是可能取到不是內容頁面
baseUrl = 'https://www.google.com/search?q='
#商業智慧+site:www.cw.com.tw
keyword = "商業智慧"
site = 'site:www.cw.com.tw'
queryString = keyword + '+' + site
startItem = 0
startParam = '&start='
suffix = '&aqs=chrome..69i57j0i333.5056j0j7&sourceid=chrome&ie=UTF-8'

#解析回傳的列表標題及連結
pageLinks = []
contain = 'article'

def requestHtml(url):
  s = requests.Session()
  r = s.get(url, headers=headers_Get)
  return BeautifulSoup(r.text, "html.parser")

def composeTitles(items):
  for item in items:
    if contain in item.a['href']:
      print(item.a['href'])
      print(item.h3.span.text)
      pageLinks.append({"title":item.h3.span.text, "link":item.a['href']})

def findLastPage(searchPages):
  lastPage = 1
  for searchPage in searchPages:
    if searchPage.a is not None:
      try:
        lastPage = int(searchPage.a.text)
        print(lastPage)
      except ValueError:
        print(searchPage.a.text + " is not integer")
  return lastPage

fullUrl = baseUrl + queryString + startParam + str(startItem) + suffix
soup = requestHtml(fullUrl)

items = soup.findAll(class_='yuRUbf')
composeTitles(items)

searchPages = soup.findAll('td')
lastPage = findLastPage(searchPages)

for i in range(2, lastPage):
  #print(i)
  startItem = (i-1)*10
  fullUrl = baseUrl + queryString + startParam + str(startItem) + suffix
  soup = requestHtml(fullUrl)
  items = soup.findAll(class_='yuRUbf')
  composeTitles(items)

#去除重複的內容
print(len(pageLinks))
pageLinks = [dict(t) for t in {tuple(d.items()) for d in pageLinks}]
print(len(pageLinks))

#排序連結
print(pageLinks)
newPageLinks = sorted(pageLinks, key=lambda k: k['link']) 
print(newPageLinks)