# -*- coding: utf-8 -*-
"""練習3-2：爬取新聞類型網站(列表+解析內容+下載圖檔).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13jhCJOx-a2uuq8hYWdx-HTQT8HN-L6VM
"""

#!pip install requests
#!pip install beautifulsoup4

import time
import re
import ntpath
import requests
from bs4 import BeautifulSoup
from google.colab import files

headers_Get = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }

#使用 Google 新聞，缺點是可能取到不是內容頁面
baseUrl = 'https://www.google.com/search?q='
#商業智慧+site:www.cw.com.tw
keyword = "商業智慧"
site = 'site:www.cw.com.tw'
queryString = keyword + '+' + site
startItem = 0
startParam = '&start='
suffix = '&aqs=chrome..69i57j0i333.5056j0j7&sourceid=chrome&ie=UTF-8'

#解析回傳的列表標題及連結
pageLinks = []
contain = 'article'

#請求網頁
def requestHtml(url):
  s = requests.Session()
  r = s.get(url, headers=headers_Get)
  return BeautifulSoup(r.text, "html.parser")

#組合列表中的標題
def composeItems(items):
  for item in items:
    if contain in item.a['href']:
      print(item.a['href'])
      print(item.h3.span.text)
      pageLinks.append({"title":item.h3.span.text, "link":item.a['href']})

#取得最後一頁
def findLastPage(searchPages):
  lastPage = 1
  for searchPage in searchPages:
    if searchPage.a is not None:
      try:
        lastPage = int(searchPage.a.text)
        print(lastPage)
      except ValueError:
        print(searchPage.a.text + " is not integer")
  return lastPage

#解析檔案路徑及檔名
def path_leaf(path):
    head, tail = ntpath.split(path)
    return tail #or ntpath.basename(head)

#以分包的方法下載檔案
def download_file(url, filename):
    # NOTE the stream=True parameter below
    try:
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(filename, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192): 
                    # If you have chunk encoded response uncomment if
                    # and set chunk_size parameter to None.
                    #if chunk: 
                    f.write(chunk)

        files.download(filename)
        return filename
    except:
        print("Error")
        return 0

fullUrl = baseUrl + queryString + startParam + str(startItem) + suffix
soup = requestHtml(fullUrl)

items = soup.findAll(class_='yuRUbf')
composeItems(items)

searchPages = soup.findAll('td')
lastPage = findLastPage(searchPages)

for i in range(2, lastPage):
  #print(i)
  startItem = (i-1)*10
  fullUrl = baseUrl + queryString + startParam + str(startItem) + suffix
  soup = requestHtml(fullUrl)
  items = soup.findAll(class_='yuRUbf')
  composeItems(items)

#去除重複的內容
print(len(pageLinks))
pageLinks = [dict(t) for t in {tuple(d.items()) for d in pageLinks}]
print(len(pageLinks))

#排序連結
print(pageLinks)
newPageLinks = sorted(pageLinks, key=lambda k: k['link']) 
print(newPageLinks)

#讀取內容頁
for pageLink in newPageLinks:
  try:
    print('讀取 >>> ' + pageLink['link'])
    soup = requestHtml(pageLink['link'])
  except:
    print('頁面錯誤。')

  #標題
  articleTitle = pageLink['title']
  try:
    #articleTitle = soup.find('title').text
    articleTitle = soup.find('div', {'class':'article__head'}).h1.text
    #print(articleTitle)
  except:
    print('找不到：標題')


  #作者
  author = 'None'
  try:
    author = soup.find(class_='author--item').span.text
    #print(author)
  except:
    try:
      author = soup.find(class_='author--item').a.text
      author = author.replace(' ', '')
      #print(author)
    except:
      print('找不到：作者')

  #發文日期
  postDate = ''
  try:
    postDate = soup.find('time', {'class':'mt5'}).text
    postDate = postDate.replace(' ', '')
    #print(postDate)
  except:
    print('找不到：發文日期')

  #前言
  preface = ''
  try:
    preface = soup.find(class_='preface').text
    #print(preface)
  except:
    print('找不到：前言')

  #內文
  content = ''
  try:
    content = soup.find(class_='article__content').text
    content = content.replace('廣告', '')
    #print(content)
  except:
    print('找不到：內文')

  #關鍵字
  keywords = ''
  try:
    keywords = soup.find('meta', {'name':'keywords'}).attrs['content']
    #print(keywords)
  except:
    print('找不到：關鍵字')

  #相關圖檔
  images = []
  try:
    imgs = soup.find_all('span', {'class':'imgzoom'})
    for img in imgs:
      url = img.attrs['data-zoom']
      filename = 'images/' + path_leaf(url)
      images.append({'url':url, 'filename': filename})
      download_file(url, filename)
    print(images)
  except:
    print('沒有附圖')

  time.sleep(1)