# -*- coding: utf-8 -*-
"""xpath_遍歷下一頁(繼取得全部內容).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U1QDLRLw_Nxme5O_HcNFvciywwlganSz
"""

#安裝相關函式庫
!pip install requests

import requests
from lxml import etree

#可以從瀏覽器的開發者工具取得 request 的 header 資訊
headers_Get = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36'
    }

#請求網頁
def requestHtml(url):
  s = requests.Session()
  r = s.get(url, headers=headers_Get)
  r.encoding = r.apparent_encoding
  #print(r.text)
  return etree.HTML(r.text)

#限定搜尋結果(指定網站)
url = 'https://www.google.com/search?q=%E7%83%8F%E5%85%8B%E8%98%AD+site%3Altn.com.tw&oq=%E7%83%8F%E5%85%8B%E8%98%AD+site%3Altn.com.tw&aqs=chrome..69i57.9993j0j4&sourceid=chrome&ie=UTF-8'
dom = requestHtml(url)

links = dom.xpath('//h3/parent::a/@href')
titles = dom.xpath('//h3/text()')

print(links)
print(titles)

#下一頁 
#//span[text()='下一頁']/parent::a/@href
#//span[contains(text(),'下一頁')]
nextPage = dom.xpath('//a[@id="pnnext"]/@href')
print(nextPage)

#這樣只能一頁
if nextPage:
  dom = requestHtml('https://www.google.com' + nextPage[0])

links += dom.xpath('//h3/parent::a/@href')
titles += dom.xpath('//h3/text()')

print(len(links))
print(len(titles))

#當有下一頁時，就一直爬取
while nextPage:
  dom = requestHtml('https://www.google.com' + nextPage[0])

  links += dom.xpath('//h3/parent::a/@href')
  titles += dom.xpath('//h3/text()')

  print(len(links))
  print(len(titles))

  nextPage = dom.xpath('//a[@id="pnnext"]/@href')

for link in links:
  print(link)

print(len(links))
#去除重複
links = list(dict.fromkeys(links))
print(len(links))

print(type(links))
print(links[0])

dom = requestHtml(links[0])
#取得標題、內文、發文日期、圖檔連結
title = dom.xpath('//h1/text()')
image = dom.xpath('//a[@class="image-popup-vertical-fit"]/@href')
publish_date = dom.xpath('//span[@class="time"]/text()')
content = dom.xpath('//span[@class="time"]/following-sibling::p/text()')
#連接 list 內的值，使用斷行 \n
content = '\n'.join(content)
#組成dict
article = {
    "title": title,
    "image": image,
    "publish_date": publish_date,
    "content": content
}

print(article)

def composeArticle(dom, link):
  #取得標題、內文、發文日期、圖檔連結
  title = dom.xpath('//h1/text()')
  image = dom.xpath('//a[@class="image-popup-vertical-fit"]/@href')
  publish_date = dom.xpath('//span[@class="time"]/text()')
  content = dom.xpath('//span[@class="time"]/following-sibling::p/text()')
  #連接 list 內的值，使用斷行 \n
  content = '\n'.join(content)

  article_image = ''
  if image:
    article_image = image[0]
  #組成dict
  return {
    "title": title[0],
    "image": article_image,
    "publish_date": publish_date[0],
    "content": content,
    "link": link
  }

from random import randint
from time import sleep

articles = []
for link in links[:10]:
  dom = requestHtml(link)

  article = composeArticle(dom, link)
  print(article)
  articles.append(article)

  sleep(randint(1,3))

import csv

csv_file = '自由時報_烏克蘭.csv'
csv_columns = ['title', 'image', 'publish_date', 'content', 'link']
try:
    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)
        writer.writeheader()
        for data in articles:
            writer.writerow(data)
except IOError:
    print("I/O error")

#將 dict 轉換成 json 字串
import json
# Serializing json  
json_object = json.dumps(articles, indent = 4, ensure_ascii=False).encode('utf-8').decode('utf-8')
print(json_object)

with open("自由時報_烏克蘭.json", "w", encoding='utf-8') as outfile:
    json.dump(articles, outfile, ensure_ascii=False)

